\documentclass[onecolumn]{ctexart}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{indentfirst}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\geometry{a4paper,scale=0.8}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\DeclareMathOperator{\rank}{rank}

\title{Notes of "Linear Mapping and Matrix Operations"}
\author{Jinxin Wang}
\date{}

\begin{document}

\maketitle

\section{Linear Mapping and Matrix}

\section{Matrix Multiplication}

\subsection{Block Matrix Multiplication}

Let $X \in M_{m \times s}, Y \in M_{s \times n}$, then $Z = XY$ is valid and $Z 
\in M_{m \times n}$. Now we divide $X$ and $Y$ into blocks by vertical and 
horizontal lines:
\[
  X = 
  \begin{pmatrix}
    X_{11} & X_{12} & \cdots & X_{1k} \\
    X_{21} & X_{22} & \cdots & X_{2k} \\
    \vdots & \vdots &  & \vdots \\
    X_{l1} & X_{l2} & \cdots & X_{lk} \\
  \end{pmatrix}
\]
in which there are $l \times k$ blocks. $X_{i1}, X_{i2}, \ldots, X_{ik}$ are 
matrices with $m_i$ rows $(m_1 + m_2 + \cdots + m_l = m)$, and $X_{1j}, X_{2j}, 
\ldots, X_{lj}$ are matrices with $s_j$ columns $(s_1 + s_2 + \cdots + s_k = s)$.

\[
  Y = 
  \begin{pmatrix}
    Y_{11} & Y_{12} & \cdots & Y_{1r} \\
    Y_{21} & Y_{22} & \cdots & Y_{2r} \\
    \vdots & \vdots &  & \vdots \\
    Y_{k1} & Y_{k2} & \cdots & Y_{kr} \\
  \end{pmatrix} 
\]
in which there are $k \times r$ blocks. $Y_{i1}, Y_{i2}, \ldots, Y_{ir}$ are 
matrices with $s_i$ rows $(s_1 + s_2 + \cdots + s_k = s)$, and $Y_{1j}, Y_{2j}, 
\ldots, Y_{kj}$ are matrices with $n_j$ columns $(n_1 + n_2 + \cdots + n_r = n)$.

By doing so, we can calculate the product $Z = XY$ by blocks. Block matrix 
multiplication follow the same rule as the regular matrix multiplication: the 
result $Z = XY$ has $l \times r$ blocks and the block $Z_{ij}$ is
\[
  Z_{ij} = X_{i1}Y_{1j} + X_{i2}Y_{2j} + \cdots + X_{ik}Y_{kj}
\]
in which the multiplication of blocks such as $X_{i\nu}Y_{\nu j}$ follows the 
rule of regular matrix multiplication. Since $X_{i\nu}$ is a $m_i \times s_\nu$ 
matrix and $Y_{\nu j}$ is a $s_\nu \times n_j$ matrix, the multiplication of 
blocks is valid and the result $Z_{ij}$ is a $m_i \times n_j$ matrix. Thus $Z = 
XY$ has $\Sigma_{i=1}^l m_i = m$ rows and $\Sigma_{j=1}^r n_j = n$ columns, 
which is consistent with the result of regular matrix multiplication.

Next we check whether $Z = XY$ produced by block matrix multiplication has the 
same entry as the regular matrix multiplication in the corresponding position. 
Let $X = (x_{ij}), Y = (y_{ij}), Z = (z_{ij})$ and let us look at $z_{i_0j_0}$.

By regular matrix multiplication,
\[
  z_{i_0j_0} = \Sigma_{t=1}^s x_{i_0 t} y_{t j_0}
\]

Suppose $z_{i_0j_0}$ is in the $i_1$-th row and the $j_1$-th column of block 
$Z_{i_2j_2}$, which is equivalent to
\[
  i_0 = m_1 + m_2 + \cdots + m_{i_2 - 1} + i_1
\]
\[
  j_0 = n_1 + n_2 + \cdots + n_{j_2 - 1} + j_1
\]
By block matrix multiplication, the block
\[
  Z_{i_2j_2} = X_{i_2 1}Y_{1 j_2} + X_{i_2 2}Y_{2 j_2} + \cdots + X_{i_2 k}Y_{k j_2}
\]
and $z_{i_0j_0}$ is the sum of the entries of $X_{i_2 \nu}Y_{\nu j_2} (1 \leq 
\nu \leq k)$ in the position of the $i_1$-th row and the $j_1$-th, and each 
entry is the inner product of the $i_1$-th row vector of $X_{i_2 \nu}$ and the 
$j_1$-th column vector of $Y_{\nu j_2}$. Since $Z$ has the same row block 
division as $X$ and the same column block division as $Y$, the $i_1$-th row 
vector of $X_{i_2 \nu}$ is a part of the $i_0$-th row vector of $X$ and the 
$j_1$-th column vector of $Y_{\nu j_2}$ is a part of the $j_0$-th column vector 
of $Y$. Hence,
\begin{equation} \label{eq:1}
  z_{i_0j_0} = \Sigma_{t=1}^{s_1} x_{i_0 t} y_{t j_0} + \Sigma_{t=s_1+1}^{s_1+s_2} x_{i_0 t} y_{t j_0} + \cdots + \Sigma_{t=s_1+s_2+\cdots+s_{k-1}}^{s_1+s_2+\cdots+s_k} x_{i_0 t} y_{t j_0} = \Sigma_{t=1}^{s} x_{i_0 t} y_{t j_0}
\end{equation}

Therefore, the block matrix multiplication and the regular matrix multiplication 
produce the same result.

\begin{remark}
  Notice that to make block matrix multiplication work, the division of columns 
  of the left matrix in a matrix multiplication must be the same as the division 
  of rows of the right matrix. As illustrated above, $X$ has $k$ columns of 
  blocks with the $j$-th column of blocks containing $s_j$ columns, and $Y$ has 
  $k$ rows of blocks with the $i$-th row of blocks containing $s_i$ rows.
\end{remark}

\begin{remark}
  From the equation (\ref{eq:1}) we can see, the essense of block matrix 
  multiplication is breaking the calculation of the entries of a product of two 
  matrices into pieces (blocks).
\end{remark}

\subsubsection{Application of Block Matrix Multiplication}

\begin{equation}
  \begin{pmatrix}
    E & A \\
    0 & E \\
  \end{pmatrix}
  \begin{pmatrix}
    A & 0 \\
    -E & B \\
  \end{pmatrix} =
  \begin{pmatrix}
    0 & AB \\
    -E & B \\
  \end{pmatrix}
\end{equation}

\begin{equation}
  \begin{pmatrix}
    E & 0 \\
    -CA^{-1} & E \\
  \end{pmatrix}
  \begin{pmatrix}
    A & B \\
    C & D \\
  \end{pmatrix}
  \begin{pmatrix}
    E & -A^{-1}B \\
    0 & E \\
  \end{pmatrix} = 
  \begin{pmatrix}
    A & 0 \\
    0 & D - CA^{-1}B \\
  \end{pmatrix}
\end{equation}

\section{Matrix Transposition}

\section{Rank of a Product of Matrices}

\section{Square Matrix}

\subsection{Commuting Matrices}

\begin{definition}[Diagonal Matrix and Scalar Matrix]
  A diagonal matrix is a a matrix in which all entries outside the main diagonal 
  are all zero.
\end{definition}

\begin{definition}[Identity Matrix]
  \begin{equation}
    E = (\delta_{ij}), \delta_{ij} = 
    \begin{cases}
      1, i = j \\
      0, i \neq j \\
    \end{cases}
  \end{equation}
\end{definition}

\begin{definition}[Scalar Matrix]
  A scalar matrix is the result of the identity matrix multiplied with a scalar.
\end{definition}

\begin{definition}[Matrix Unit]
  A matrix unit is a matrix with only one nonzero entry with value 1. The matrix 
  unit with the nonzero entry in the $i$-th row and $j$-th column is denoted as 
  $E_{ij}$
\end{definition}
\begin{remark}
  A matrix unit is not necessarily a square matrix.
\end{remark}

\begin{definition}[Commuting Matrices]
  Given two matrices $A, B \in M_n(\mathbb{R})$, $A$ and $B$ are said to commute 
  if $AB = BA$, or equivalently their commutator $\lbrack A, B \rbrack = AB - BA 
  = 0$.
\end{definition}
\begin{remark}
  Notice that commuting matrices must be square matrix, because if $A \in M_{s 
  \times n}, B \in M_{n \times r}, s \neq r$, then $AB \in M_{s \times r}, BA 
  \in M_{r \times s}$, hence it is invalid to compare them.
\end{remark}

\begin{theorem}
  If a matrix $A \in M_n(\mathbb{R})$ commutes with $\forall B \in 
  M_n(\mathbb{R})$, then $A$ is a scalar matrix.
\end{theorem}
\begin{proof}
  Hint:
  \begin{itemize}
    \item If $AE_{12} = E_{12}A$, then $\forall k \neq 1, a_{k1} = 0$, and 
    $\forall k \neq 2, a_{2k} = 0$, and $a_{11} = a_{22}$.
    \item If $AE_{ij} = E_{ij}A$, then $\forall k \neq i, a_{ki} = 0$, and 
    $\forall k \neq j, a_{jk} = 0$, and $a_{ii} = a_{jj}$.
    \item Consider $\forall B \in M_n{\mathbb{R}}, AB = BA$.
  \end{itemize}
\end{proof}
\begin{remark}
  When proving a property applies to any matrix in $M_{m \times n}$, one method 
  is to consider all matrix units in $M_{m \times n}$, since the set of matrix 
  units is a basis of $M_{m \times n}$.
\end{remark}

\subsection{Inverse Matrix}
\begin{lemma}[Uniqueness of Inverse Matrix]
  \[
    A' = A'E = A'(AA'') = (A'A)A'' = A''
  \]
\end{lemma}
\begin{definition}[Inverse Matrix]
  
\end{definition}

\begin{definition}[Non-degenerate Matrix and Degenerate Matrix]
  A matrix $A \in M_n(\mathbb{R})$ is non-degenerate if $\rank A = n$. $A$ is 
  degenerate if $\rank A < n$.
\end{definition}
\begin{remark}
  We only talk about non-degenerate matrices and degenerate matrices when it 
  comes to square matrices.
\end{remark}

\begin{theorem}
  $A \in M_n(\mathbb{R})$ is non-degenerate if and only if $A$ is inversible.
\end{theorem}
\begin{proof}
  Hint: \\
  $\Leftarrow$: Use the rank of the product of two matrices. \\
  $\Rightarrow$: Use the unqiueness of the solution of $AX = 0$, and the 
  transpose of the product of two matrices.
\end{proof}

\begin{corollary}
  If $A \in M_n{\mathbb{R}}$ is inversible, then $A^T$ is inversible, and 
  $(A^T)^{-1} = (A^{-1})^T$.
\end{corollary}

\begin{corollary}
  If $B \in M_m(\mathbb{R})$ and $C \in M_n(\mathbb{R})$ are non-degenerate, 
  $\forall A \in M_{m \times n}(\mathbb{R})$, it holds that
  \[
    \rank BAC = \rank A
  \]
\end{corollary}

\begin{corollary}
  If $A, B \in M_n(\mathbb{R})$, and $AB = E \vee BA = E$, then $B = A^{-1}$.
\end{corollary}

\begin{corollary}
  If $A, B, \ldots, C, D \in M_n(\mathbb{R})$ are non-degenerate, then 
  $AB \cdots CD$ is non-degenerate, and its inverse matrix is
  \[
    (AB \cdots CD)^{-1} = D^{-1} C^{-1} \cdots B^{-1} A^{-1}
  \]
\end{corollary}

\subsection{Calculation of Powers of a Matrix}

\begin{example}[Powers of a Scalar Matrix]
  
\end{example}

\begin{example}
  \[
    A = 
    \begin{pmatrix}
      a & c \\
      0 & b \\
    \end{pmatrix}
  \]
\end{example}

\begin{example}
  \[
    A = 
    \begin{pmatrix}
      0 & 1 \\
      1 & 1 \\
    \end{pmatrix}
  \]
\end{example}

\section{Equivalence Class of Matrices}

\begin{definition}[Elementary Matrix]
  We call a matrix which can be obtained by doing one time of elementary row 
  operation or column operation to the identity matrix as an elementary matrix.
\end{definition}

There are three kinds of elementary matrices, corresponding to the three kinds 
of elementary operations:
\begin{equation}
  \begin{split}
    F_{s,t} &= 
    \begin{pmatrix}
      1 \\
        & \ddots & \\
        &        & 0 &   & 1 \\
        &        &   & 1 &   \\
        &        & 1 &   & 0 \\
        &        &   &   &   & \ddots &   \\
        &        &   &   &   &        & 1 \\
    \end{pmatrix}
  \end{split}
\end{equation}
\begin{remark}
  This matrix can be derived by exchanging the $s$-th column and the $t$-th 
  column of $E$, or equivalently exchanging the $s$-th row and the $t$-th row of 
  $E$.
\end{remark}
\begin{equation}
  \begin{split}
    F_{s,t}(\lambda) &= E + \lambda E_{s,t} \\
                     &= 
                     \begin{pmatrix}
                      1 \\
                        & \ddots \\
                        &        & 1 & \cdots & \lambda & \cdots \\
                        &        &   & \ddots &         &        \\
                        &        &   &        & 1       &        \\
                        &        &   &        &         & 1      \\
                     \end{pmatrix}
  \end{split}
\end{equation}
\begin{remark}
  This matrix can be derived by adding the $s$-th column multiplied by $\lambda$ 
  to the $t$-th column, or equivalently adding the $t$-th row multiplied by 
  $\lambda$ to the $s$-th row.
\end{remark}
\begin{equation}
  \begin{split}
    F_{s}(\lambda) &= E + (\lambda - 1)E_{s,s} \\
                   &= 
                   \begin{pmatrix}
                     1 \\
                       & \ddots \\
                       &        & \lambda \\
                       &        &         & \ddots \\
                       &        &         &        & 1
                   \end{pmatrix}
  \end{split}
\end{equation}
\begin{remark}
  This matrix can be derived by multiplying the $s$-th row by $\lambda$, or 
  equivalently multiplying the $s$-th column by $\lambda$.
\end{remark}

\begin{proposition}
  Given an elementary $F$ 
\end{proposition}

\begin{theorem}
  $M_{m \times n}(\mathbb{R})$ has a partition of $\min(m, n) + 1$ equivalence 
  classes. All matrices with its rank as $r$ including the representative 
  element is in a equivalence class.
\end{theorem}

\begin{corollary}
  Every non-degenerate matrix $A \in M_n(\mathbb{R})$ can be expressed as the 
  product of elementary matrices.
\end{corollary}

\section{Calculation of Inverse Matrix}

\section{Solution Space}

\end{document}